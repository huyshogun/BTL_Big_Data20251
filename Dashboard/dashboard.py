import streamlit as st
import pandas as pd
import plotly.express as px
import os
import sys

# C·∫•u h√¨nh PySpark ƒë·ªÉ ch·∫°y ng·∫ßm trong Streamlit
# (C·∫ßn thi·∫øt ƒë·ªÉ load model v√† connect Cassandra)
os.environ['HADOOP_HOME'] = "D:\\hadoop"
sys.path.append("D:\\hadoop\\bin")
os.environ['JAVA_HOME'] = "C:\\Program Files\\Eclipse Adoptium\\jdk-11.0.29.7-hotspot"
os.environ['PATH'] = os.environ['JAVA_HOME'] + "\\bin;" + os.environ['PATH']

from pyspark.sql import SparkSession
from pyspark.ml import PipelineModel
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType

# ==========================================
# 1. SETUP SPARK SESSION (CACHE RESOURCE)
# ==========================================
@st.cache_resource
def get_spark_session():
    """Kh·ªüi t·∫°o Spark 1 l·∫ßn duy nh·∫•t cho c·∫£ app"""
    return SparkSession.builder \
    .appName("HanoiHousePrice_Training_Batch") \
    .master("local[*]") \
    \
    .config("spark.driver.host", "127.0.0.1") \
    .config("spark.driver.bindAddress", "127.0.0.1") \
    \
    .config("spark.network.timeout", "300s") \
    .config("spark.executor.heartbeatInterval", "60s") \
    \
    .config("spark.hadoop.fs.defaultFS", "hdfs://localhost:9000") \
    .config("spark.hadoop.fs.hdfs.impl", "org.apache.hadoop.hdfs.DistributedFileSystem") \
    .config("spark.hadoop.dfs.client.use.datanode.hostname", "true") \
    \
    .config("spark.jars.packages", "com.datastax.spark:spark-cassandra-connector_2.12:3.3.0") \
    .config("spark.cassandra.connection.host", "localhost") \
    .config("spark.cassandra.connection.port", "9042") \
    .getOrCreate()

@st.cache_resource
def load_trained_model(_spark):
    """Load model ƒë√£ train t·ª´ disk"""
    try:
        model_path = "file:///D:/BTL_Big_Data/model"
        return PipelineModel.load(model_path)
    except Exception as e:
        return None

# Kh·ªüi t·∫°o
spark = get_spark_session()
model = load_trained_model(spark)

# ==========================================
# 2. H√ÄM TRUY V·∫§N D·ªÆ LI·ªÜU
# ==========================================
def get_dashboard_stats():
    """L·∫•y th·ªëng k√™ t·ªïng h·ª£p t·ª´ Cassandra (B·∫£ng data2)"""
    try:
        # ƒê·ªçc b·∫£ng data2 t·ª´ Cassandra
        df_cass = spark.read \
            .format("org.apache.spark.sql.cassandra") \
            .options(table="data2", keyspace="finaldata1") \
            .load()
        
        # T·∫°o View ƒë·ªÉ query SQL cho l·∫π
        df_cass.createOrReplaceTempView("houses")
        
        # Query th·ªëng k√™
        stats = spark.sql("""
            SELECT 
                city, 
                COUNT(*) as count, 
                AVG(price) as avg_price, 
                AVG(livingarea) as avg_area,
                MAX(price) as max_price
            FROM houses 
            WHERE city IS NOT NULL 
            GROUP BY city
        """).toPandas()
        
        return stats
    except Exception as e:
        st.error(f"L·ªói ƒë·ªçc Cassandra: {e}")
        return pd.DataFrame()

def predict_custom_house(input_data):
    """D·ª± ƒëo√°n gi√° cho input nh·∫≠p tay"""
    if model is None:
        return 0.0
    
    # T·∫°o DataFrame t·ª´ input dictionary
    # C·∫ßn ƒë√∫ng schema nh∆∞ l√∫c train
    schema = StructType([
        StructField("city", StringType(), True),
        StructField("homeType", StringType(), True),
        StructField("newConstructionType", StringType(), True),
        StructField("lotAreaValue", DoubleType(), True),
        StructField("bathrooms", DoubleType(), True),
        StructField("bedrooms", DoubleType(), True),
        StructField("livingArea", DoubleType(), True),
        StructField("isFeatured", DoubleType(), True),
        StructField("isShowcaseListing", DoubleType(), True),
        StructField("description", StringType(), True) # ƒê·ªÉ tr√≠ch xu·∫•t NLP features
        # C√°c c·ªôt NLP s·∫Ω ƒë∆∞·ª£c t·∫°o trong Pipeline (n·∫øu Pipeline bao g·ªìm b∆∞·ªõc ƒë√≥)
        # L∆∞u √Ω: N·∫øu b∆∞·ªõc t·∫°o c·ªôt NLP n·∫±m NGO√ÄI Pipeline (nh∆∞ trong sparkML.py c≈©),
        # b·∫°n ph·∫£i t·ª± t·∫°o c·ªôt ƒë√≥ ·ªü ƒë√¢y tr∆∞·ªõc khi ƒë∆∞a v√†o model.transform
    ])
    
    # ·ªû file sparkML.py m·ªõi, t√¥i ƒë√£ gi·∫£ ƒë·ªãnh c√°c b∆∞·ªõc NLP n·∫±m ngo√†i Pipeline cho ƒë∆°n gi·∫£n.
    # N√™n ·ªü ƒë√¢y ta c·∫ßn t√°i t·∫°o logic feature engineering c∆° b·∫£n
    rows = [input_data]
    df_input = spark.createDataFrame(rows) # Schema t·ª± suy di·ªÖn ho·∫∑c √©p ki·ªÉu sau
    
    # T√°i t·∫°o logic NLP c∆° b·∫£n (gi·ªëng sparkML.py)
    from pyspark.sql.functions import lit, col, lower, when
    df_processed = df_input \
        .withColumn("desc_lower", lower(col("description"))) \
        .withColumn("has_red_book", when(col("desc_lower").rlike("s·ªï ƒë·ªè|s·ªï h·ªìng"), 1.0).otherwise(0.0)) \
        .withColumn("is_street_front", when(col("desc_lower").rlike("m·∫∑t ti·ªÅn|kinh doanh"), 1.0).otherwise(0.0)) \
        .withColumn("is_wide_alley", when(col("desc_lower").rlike("xe h∆°i|oto"), 1.0).otherwise(0.0)) \
        .withColumn("isFeatured", lit(0.0)) \
        .withColumn("isShowcaseListing", lit(0.0))
        
    # √âp ki·ªÉu cho kh·ªõp model
    numeric_cols = ['lotAreaValue', 'bathrooms', 'bedrooms', 'livingArea']
    for c in numeric_cols:
        df_processed = df_processed.withColumn(c, col(c).cast(DoubleType()))

    # D·ª± ƒëo√°n
    prediction = model.transform(df_processed)
    return prediction.select("prediction").collect()[0][0]

# ==========================================
# 3. GIAO DI·ªÜN STREAMLIT
# ==========================================
st.set_page_config(page_title="Real Estate AI Dashboard", layout="wide")

st.title("üèôÔ∏è Real Estate AI Analytics Center")
st.markdown("---")

# --- PH·∫¶N 1: TH·ªêNG K√ä T·ªîNG H·ª¢P (READ CASSANDRA) ---
st.header("1. Th·ªã tr∆∞·ªùng T·ªïng quan")
if st.button("üîÑ C·∫≠p nh·∫≠t d·ªØ li·ªáu"):
    st.cache_data.clear()

stats_df = get_dashboard_stats()

if not stats_df.empty:
    col1, col2, col3 = st.columns(3)
    col1.metric("T·ªïng s·ªë tin ƒëƒÉng", f"{stats_df['count'].sum():,}")
    col1.metric("Gi√° trung b√¨nh to√†n th·ªã tr∆∞·ªùng", f"{stats_df['avg_price'].mean()/1e9:,.2f} T·ª∑")
    
    # Bi·ªÉu ƒë·ªì gi√° theo th√†nh ph·ªë
    fig = px.bar(stats_df, x='city', y='avg_price', 
                 title="Gi√° trung b√¨nh theo Qu·∫≠n/Huy·ªán",
                 color='count', labels={'avg_price': 'Gi√° TB (VNƒê)'})
    st.plotly_chart(fig, use_container_width=True)
else:
    st.warning("Ch∆∞a c√≥ d·ªØ li·ªáu trong Cassandra.")

st.markdown("---")

# --- PH·∫¶N 2: D·ª∞ ƒêO√ÅN GI√Å (INPUT FORM) ---
st.header("2. ƒê·ªãnh gi√° Nh√† (AI Prediction)")

c1, c2, c3 = st.columns(3)
with c1:
    inp_city = st.selectbox("Qu·∫≠n/Huy·ªán", ["H√† ƒê√¥ng", "C·∫ßu Gi·∫•y", "ƒê·ªëng ƒêa", "Thanh Xu√¢n", "Ho√†ng Mai"])
    inp_area = st.number_input("Di·ªán t√≠ch (m2)", min_value=10.0, value=50.0)
with c2:
    inp_bed = st.number_input("S·ªë ph√≤ng ng·ªß", 1, 10, 2)
    inp_bath = st.number_input("S·ªë ph√≤ng t·∫Øm", 1, 10, 2)
with c3:
    inp_desc = st.text_area("M√¥ t·∫£ (V√≠ d·ª•: Nh√† m·∫∑t ti·ªÅn, c√≥ s·ªï ƒë·ªè)", "Nh√† c√≥ s·ªï ƒë·ªè, ng√µ xe h∆°i")

if st.button("üîÆ D·ª± ƒëo√°n gi√° ngay"):
    if model:
        with st.spinner("AI ƒëang t√≠nh to√°n..."):
            input_data = {
                "city": inp_city,
                "homeType": "UNKNOWN", # Gi√° tr·ªã m·∫∑c ƒë·ªãnh
                "newConstructionType": "UNKNOWN",
                "lotAreaValue": inp_area,
                "livingArea": inp_area,
                "bathrooms": inp_bath,
                "bedrooms": inp_bed,
                "description": inp_desc
            }
            pred_price = predict_custom_house(input_data)
            st.success(f"üí∞ Gi√° d·ª± ƒëo√°n: **{pred_price/1e9:,.2f} T·ª∑ VNƒê**")
    else:
        st.error("Ch∆∞a load ƒë∆∞·ª£c Model. H√£y ch·∫°y file sparkML.py ƒë·ªÉ train tr∆∞·ªõc!")

# --- PH·∫¶N 3: XEM G·ª¢I √ù (READ CASSANDRA RECOMMENDATIONS) ---
st.markdown("---")
st.header("3. G·ª£i √Ω Nh√† T∆∞∆°ng t·ª±")

# Nh·∫≠p ID cƒÉn nh√† mu·ªën xem (trong th·ª±c t·∫ø s·∫Ω click t·ª´ danh s√°ch)
target_id = st.text_input("Nh·∫≠p m√£ cƒÉn nh√† (ZPID) ƒë·ªÉ xem g·ª£i √Ω:", "")

if target_id:
    # Query b·∫£ng recommendations t·ª´ Cassandra
    try:
        rec_df_spark = spark.read \
            .format("org.apache.spark.sql.cassandra") \
            .options(table="recommendations", keyspace="finaldata1") \
            .load() \
            .filter(f"source_id = '{target_id}'")
        
        recs = rec_df_spark.toPandas()
        
        if not recs.empty:
            st.write(f"T√¨m th·∫•y {len(recs)} cƒÉn t∆∞∆°ng t·ª±:")
            st.dataframe(recs[['target_id', 'city', 'price', 'livingarea', 'distance']])
        else:
            st.info("Kh√¥ng t√¨m th·∫•y g·ª£i √Ω cho cƒÉn n√†y (C√≥ th·ªÉ do ch∆∞a ch·∫°y Batch Job).")
            
    except Exception as e:
        st.error(f"L·ªói truy v·∫•n g·ª£i √Ω: {e}")