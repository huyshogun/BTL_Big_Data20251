from pyspark.sql import SparkSession
from pyspark.ml.regression import GBTRegressor
from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder, Imputer, BucketedRandomProjectionLSH
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml import Pipeline
from pyspark.sql.functions import col, abs, lower, when, lit
from pyspark.sql.types import DoubleType, IntegerType, StringType

import os
import sys

# ==========================================
# 1. Cáº¤U HÃŒNH MÃ”I TRÆ¯á»œNG
# ==========================================
os.environ['HADOOP_HOME'] = "D:\\hadoop" 
sys.path.append("D:\\hadoop\\bin")
os.environ['JAVA_HOME'] = "C:\\Program Files\\Eclipse Adoptium\\jdk-11.0.29.7-hotspot"
os.environ['PATH'] = os.environ['JAVA_HOME'] + "\\bin;" + os.environ['PATH']

spark = SparkSession.builder \
    .appName("HanoiHousePrice_Advanced_NLP_RecSys") \
    .master("local[*]") \
    .config("spark.jars.packages", "com.datastax.spark:spark-cassandra-connector_2.12:3.3.0") \
    .config("spark.cassandra.connection.host", "localhost") \
    .config("spark.cassandra.connection.port", "9042") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

# ==========================================
# 2. Äá»ŒC Dá»® LIá»†U Tá»ª CASSANDRA
# ==========================================
print(">>> [1/6] Loading data from Cassandra...")
try:
    df_read = spark.read \
        .format("org.apache.spark.sql.cassandra") \
        .options(table="data2", keyspace="finaldata1") \
        .load()
except Exception as e:
    print(f"!!! Lá»—i Ä‘á»c Cassandra: {e}")
    sys.exit(1)

# ==========================================
# 3. FEATURE ENGINEERING (NLP + CLEANING)
# ==========================================
print(">>> [2/6] Advanced Feature Engineering (NLP Extraction)...")

# 3.1: Xá»­ lÃ½ cá»™t Description (Náº¿u null thÃ¬ Ä‘á»ƒ chuá»—i rá»—ng)
# LÆ°u Ã½: Náº¿u Cassandra chÆ°a cÃ³ cá»™t description, Spark sáº½ bÃ¡o lá»—i. HÃ£y cháº¯c cháº¯n Ä‘Ã£ cháº¡y BÆ°á»›c 1.
if "description" not in df_read.columns:
    print("!!! Cáº£nh bÃ¡o: KhÃ´ng tÃ¬m tháº¥y cá»™t 'description'. Äang táº¡o cá»™t giáº£ láº­p...")
    df_read = df_read.withColumn("description", lit("KhÃ´ng cÃ³ mÃ´ táº£"))

df_nlp = df_read.fillna({"description": ""})

# 3.2: NLP - TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng tá»« vÄƒn báº£n (Keyword Extraction)
# Chuyá»ƒn vá» chá»¯ thÆ°á»ng Ä‘á»ƒ dá»… so sÃ¡nh
df_nlp = df_nlp.withColumn("desc_lower", lower(col("description")))

# Táº¡o 3 tÃ­nh nÄƒng má»›i (Binary Features)
df_processed = df_nlp \
    .withColumn("has_red_book", when(col("desc_lower").rlike("sá»• Ä‘á»|sá»• há»“ng|chÃ­nh chá»§"), 1).otherwise(0)) \
    .withColumn("is_street_front", when(col("desc_lower").rlike("máº·t tiá»n|máº·t phá»‘|kinh doanh"), 1).otherwise(0)) \
    .withColumn("is_wide_alley", when(col("desc_lower").rlike("xe hÆ¡i|Ã´ tÃ´|oto"), 1).otherwise(0))

print("    -> ÄÃ£ trÃ­ch xuáº¥t xong: has_red_book, is_street_front, is_wide_alley")

# ==========================================
# 4. CHUáº¨N Bá»Š Dá»® LIá»†U TRAIN
# ==========================================
print(">>> [3/6] Cleaning & Type Casting...")

categorical_columns = ['city', 'hometype', 'newconstructiontype']
real_numeric_cols = ['lotareavalue', 'bathrooms', 'bedrooms', 'livingarea']
# ThÃªm cÃ¡c cá»™t NLP má»›i vÃ o danh sÃ¡ch cá»™t Boolean
boolean_cols = ['isfeatured', 'isshowcaselisting', 'listingsubtype_is_newhome', 
                'has_red_book', 'is_street_front', 'is_wide_alley']

all_numeric_cols = real_numeric_cols + boolean_cols

# Fill NA
df_processed = df_processed.fillna({c: 'UNKNOWN' for c in categorical_columns})
fill_values = {c: 0 for c in boolean_cols}
fill_values.update({c: 0.0 for c in real_numeric_cols})
df_processed = df_processed.fillna(fill_values)

# Cast types
for col_name in boolean_cols:
    df_processed = df_processed.withColumn(col_name, col(col_name).cast(IntegerType()).cast(DoubleType()))
for col_name in real_numeric_cols:
    df_processed = df_processed.withColumn(col_name, col(col_name).cast(DoubleType()))

df_processed = df_processed.withColumn("price", col("price").cast(DoubleType()))

# ==========================================
# 5. XÃ‚Y Dá»°NG PIPELINE (GBT REGRESSOR)
# ==========================================
print(">>> [4/6] Building Pipeline & Training...")

stages = []

# Imputer
imputer = Imputer(inputCols=all_numeric_cols, outputCols=[f"{c}_imputed" for c in all_numeric_cols]).setStrategy("mean")
stages.append(imputer)

# Encoder
for col_name in categorical_columns:
    indexer = StringIndexer(inputCol=col_name, outputCol=f"{col_name}_index", handleInvalid="keep")
    encoder = OneHotEncoder(inputCol=f"{col_name}_index", outputCol=f"{col_name}_vec")
    stages += [indexer, encoder]

# Vector Assembler
assembler_inputs = [f"{c}_vec" for c in categorical_columns] + [f"{c}_imputed" for c in all_numeric_cols]
assembler = VectorAssembler(inputCols=assembler_inputs, outputCol="features")
stages.append(assembler)

# Model GBT
gbt = GBTRegressor(featuresCol="features", labelCol="price", maxIter=50, stepSize=0.1, seed=42)
stages.append(gbt)

pipeline = Pipeline(stages=stages)

# Split & Train
(train_data, test_data) = df_processed.randomSplit([0.8, 0.2], seed=42)
train_data.cache()
test_data.cache()

model = pipeline.fit(train_data)
predictions = model.transform(test_data)

# Evaluate
evaluator_r2 = RegressionEvaluator(labelCol="price", predictionCol="prediction", metricName="r2")
r2 = evaluator_r2.evaluate(predictions)
print(f"\nğŸ“Š [Káº¾T QUáº¢ Dá»° BÃO] R2 Score (vá»›i NLP Features): {r2:.4f}")

# ==========================================
# 6. Há»† THá»NG Gá»¢I Ã (RECOMMENDATION SYSTEM)
# ==========================================
print("\n" + "="*50)
print("ğŸ  TÃNH NÄ‚NG Má»šI: Há»† THá»NG Gá»¢I Ã NHÃ€ TÆ¯Æ NG Tá»° (LSH)")
print("="*50)

# BÆ°á»›c 6.1: Chuáº©n bá»‹ model LSH (Locality Sensitive Hashing)
# LSH giÃºp tÃ¬m kiáº¿m vector tÆ°Æ¡ng tá»± cá»±c nhanh trong khÃ´ng gian nhiá»u chiá»u
# ChÃºng ta dÃ¹ng vector 'features' Ä‘Ã£ Ä‘Æ°á»£c táº¡o ra bá»Ÿi Pipeline trÃªn
# Tuy nhiÃªn, Pipeline model tráº£ vá» 'predictions' Ä‘Ã£ cÃ³ cá»™t 'features', ta dÃ¹ng luÃ´n nÃ³.

# Láº¥y dá»¯ liá»‡u Ä‘Ã£ transform xong (chá»©a cá»™t features)
df_vectorized = model.transform(df_processed)

# Khá»Ÿi táº¡o LSH
# bucketLength: Äá»™ rá»™ng cá»§a bucket (cÃ ng nhá» cÃ ng chÃ­nh xÃ¡c nhÆ°ng cháº­m)
# numHashTables: Sá»‘ lÆ°á»£ng báº£ng bÄƒm
brp = BucketedRandomProjectionLSH(inputCol="features", outputCol="hashes", bucketLength=100.0, numHashTables=5)

# Fit mÃ´ hÃ¬nh LSH
print(">>> Training LSH Model...")
model_lsh = brp.fit(df_vectorized)

# BÆ°á»›c 6.2: Demo tÃ¬m nhÃ  tÆ°Æ¡ng tá»±
# Láº¥y cÄƒn nhÃ  Ä‘áº§u tiÃªn trong táº­p Test lÃ m vÃ­ dá»¥ "NhÃ  Ä‘ang xem"
print(">>> Äang chá»n má»™t cÄƒn nhÃ  máº«u tá»« táº­p Test...")
target_house = model.transform(test_data).first()
target_features = target_house['features']

print(f"--- CÄƒn nhÃ  Ä‘ang xem ---")
print(f"Äá»‹a chá»‰: {target_house['city']}")
print(f"GiÃ¡: {target_house['price']/1e9:.2f} Tá»· | DT: {target_house['livingarea']} m2 | PN: {target_house['bedrooms']}")
if target_house['has_red_book'] == 1: print("âœ… CÃ³ Sá»• Ä‘á»/ChÃ­nh chá»§")
if target_house['is_street_front'] == 1: print("âœ… Máº·t tiá»n/Kinh doanh")

# BÆ°á»›c 6.3: TÃ¬m 5 hÃ ng xÃ³m gáº§n nháº¥t (Nearest Neighbors)
print("\n>>> ğŸ” Top 5 CÄƒn nhÃ  tÆ°Æ¡ng tá»± nháº¥t (Dá»±a trÃªn AI):")
similar_houses = model_lsh.approxNearestNeighbors(df_vectorized, target_features, 5)

# Hiá»ƒn thá»‹ káº¿t quáº£
print(">>> [6/6] Exporting Recommendation Results...")

# 1. TÃ­nh toÃ¡n thÃªm cá»™t "Do_Giong_Nhau" (Similarity Score) ngay trong Spark
# CÃ´ng thá»©c: Khoáº£ng cÃ¡ch cÃ ng nhá» -> Äá»™ giá»‘ng cÃ ng cao (Max 100%)
# Cast sang Integer Ä‘á»ƒ sá»‘ Ä‘áº¹p (VD: 95, 80...)
rec_final = similar_houses \
    .withColumn("Do_Giong_Nhau", (1 / (1 + col("distCol")) * 100).cast("int")) \
    .withColumn("price_Ty", (col("price")/1e9).cast("decimal(10,2)")) \
    .orderBy("distCol") # Sáº¯p xáº¿p Ä‘á»ƒ cÄƒn giá»‘ng nháº¥t lÃªn Ä‘áº§u

# 2. In ra mÃ n hÃ¬nh Ä‘á»ƒ kiá»ƒm tra (Debug)
print("--- Preview dá»¯ liá»‡u gá»£i Ã½ ---")
rec_final.select("city", "livingarea", "price_Ty", "has_red_book", "is_street_front", "Do_Giong_Nhau").show()

# 3. Xuáº¥t ra CSV
try:
    # Chá»n Ä‘áº§y Ä‘á»§ cÃ¡c cá»™t mÃ  Dashboard cáº§n
    rec_export = rec_final.select(
        "city", 
        "livingarea", 
        "bedrooms", 
        "price", 
        "has_red_book", 
        "is_street_front", 
        "distCol",
        "Do_Giong_Nhau" # ThÃªm cá»™t nÃ y vÃ o file
    )
    
    # Chuyá»ƒn sang Pandas & LÆ°u CSV
    rec_pandas = rec_export.toPandas()
    rec_pandas.to_csv("D:/BTL_Big_Data/Big_Data_20251/recommendation_results.csv", index=False)
    print("-> ÄÃ£ lÆ°u file gá»£i Ã½ thÃ nh cÃ´ng: D:/BTL_Big_Data/recommendation_results.csv")
    
except Exception as e:
    print(f"!!! Lá»—i xuáº¥t file gá»£i Ã½: {e}")
    
print("ChÃº thÃ­ch: 'distCol' cÃ ng nhá» nghÄ©a lÃ  cÃ ng giá»‘ng cÄƒn gá»‘c.")

# ==========================================
# 7. XUáº¤T FILE CHO DASHBOARD
# ==========================================
# (Äoáº¡n code xuáº¥t CSV cÅ© giá»¯ nguyÃªn á»Ÿ Ä‘Ã¢y Ä‘á»ƒ váº½ dashboard)
output_df = predictions.select("city", "livingarea", "bedrooms", "bathrooms", "price", "prediction")
pandas_df = output_df.toPandas()
pandas_df.to_csv("D:/BTL_Big_Data/Big_Data_20251/prediction_results.csv", index=False)
print("\n>>> ÄÃ£ xuáº¥t file káº¿t quáº£ Dashboard.")

spark.stop()